{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pifpafpose_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detect\n",
    "\n",
    "> Pose Detector class based on Open PifPaf and some pose modification tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# These torch versions are entirely out of date, unfortunately\n",
    "#%pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#%pip install openpifpaf==0.12.14\n",
    "#%pip install opencv-python\n",
    "\n",
    "#!wget https://github.com/vita-epfl/openpifpaf-torchhub/releases/download/v0.11.0/shufflenetv2k30w-200510-104256-cif-caf-caf25-o10s-0b5ba06f.pkl\n",
    "#!wget https://github.com/DuncanZauss/openpifpaf_assets/releases/download/v0.1.0/center_ref_shufflenetv2k30.pkl.epoch350\n",
    "#!wget http://github.com/DuncanZauss/openpifpaf_assets/releases/download/v0.1.0/sk16_wholebody.pkl\n",
    "#!wget http://github.com/DuncanZauss/openpifpaf_assets/releases/download/v0.1.0/sk30_wholebody.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import openpifpaf\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "openpifpaf.show.Canvas.show = True\n",
    "openpifpaf.show.Canvas.image_min_dpi = 200\n",
    "\n",
    "class Detector:\n",
    "    \"\"\"Given a still image (or video frame), finds poses.\n",
    "    \n",
    "    Attributes:  \n",
    "      device: PyTorch computing resource (GPU or CPU)  \n",
    "      net: Pose detection neural network model  \n",
    "      processor: Pose detection image processor  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.device = torch.device('cuda')  # if cuda is available\n",
    "        except:\n",
    "            self.device = torch.device('cpu')\n",
    "            \n",
    "        #self.predictor = openpifpaf.Predictor(checkpoint='shufflenetv2k30-wholebody')\n",
    "        self.predictor = openpifpaf.Predictor(checkpoint='shufflenetv2k30')\n",
    "\n",
    "    def detect_image(self, image_path, viz=False):\n",
    "        \"\"\" Applies the pose detection model to a single image file. Returns detections. \"\"\"\n",
    "        pil_im = PIL.Image.open(image_path)\n",
    "        image_array = np.asarray(pil_im)\n",
    "        \n",
    "        detections, gt_anns, image_meta = self.predictor.pil_image(pil_im)\n",
    "        \n",
    "        if viz:\n",
    "            self.plot_poses(detections, image_array, show=True)\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def __detect_pil_image__(self, pil_im):\n",
    "        detections, gt_anns, image_meta = self.predictor.pil_image(pil_im)\n",
    "        return detections\n",
    "        \n",
    "    \n",
    "    def plot_poses(self, detections, image_array=None, show=True, savepath=\"\", show_axis=False):\n",
    "\n",
    "        skeleton_painter = openpifpaf.show.painters.KeypointPainter()\n",
    "        # These are some of the viz parameters\n",
    "        #skeleton_painter = openpifpaf.show.painters.KeypointPainter(show_box=True, marker_size=1, line_width=6, highlight_invisible=True, show_joint_scales=True)\n",
    "        \n",
    "        # This is the most straightforward way to draw the skeletons and annotations, but\n",
    "        # it doesn't provide access to the viz parameters and doesn't allow the background\n",
    "        # to be blank\n",
    "        #annotation_painter = openpifpaf.show.AnnotationPainter()\n",
    "        #with openpifpaf.show.image_canvas(image_array) as ax:\n",
    "        #    annotation_painter.annotations(ax, detections) \n",
    "\n",
    "        vis_detections = []\n",
    "        \n",
    "        if hasattr(detections, 'data'):\n",
    "            vis_detections = [detections]\n",
    "        else:\n",
    "            for pose in detections:\n",
    "                if pose.data.shape[0] == 0:\n",
    "                    continue\n",
    "                vis_detections.append(pose)\n",
    "                \n",
    "        with openpifpaf.show.canvas() as ax:\n",
    "            if image_array is not None:\n",
    "                ax.imshow(image_array)\n",
    "            else:\n",
    "                # If there isn't a background image, the poses are plotted with the\n",
    "                # origin in the bottom left, rather than top left\n",
    "                ax.set_aspect('equal')\n",
    "                ax.invert_yaxis()\n",
    "            for detection in vis_detections:\n",
    "                skeleton_painter.annotation(ax, detection)\n",
    "            if not show_axis:\n",
    "                ax.set_axis_off()\n",
    "            fig = ax.get_figure()\n",
    "            fig.set_constrained_layout(True)\n",
    "            \n",
    "            if savepath != \"\":\n",
    "                fig.savefig(savepath)\n",
    "\n",
    "        return fig\n",
    "    \n",
    "    def overlay_poses(self, image_array, figures_frame, show=False, source_figure='figures', show_axis=False, savepath=\"\"):\n",
    "        return self.plot_poses(figures_frame[source_figure], image_array, show=show, show_axis=show_axis, savepath=savepath)\n",
    "    \n",
    "    \n",
    "    def detect_video(self, video_file, start_seconds=0.0, end_seconds=0.0, max_frames=0, seconds_to_skip=0.0, images_too=False, write_images=False, folder_name='video_folder'):\n",
    "        \"\"\" Given a video file, extracts video frames as images at `seconds_to_skip` intervals,\n",
    "            from `start_seconds` to `end_seconds`, and runs `__detect_one_or_more_images__()` on each.\n",
    "            Returns a list of frame pose data items, which are dictionaries with the following elements:\n",
    "            { 'frame_id': <the frame's position in this list (not in the entire video, if seconds_to_skip != 0)>, \n",
    "              'time': <the frame's timecode within the excerpt (not within the full video, if start_seconds != 0)>,\n",
    "              'figures': [<OpenPifPaf pose detection objects> for all figures detected in the frame]\n",
    "              <OPTIONAL> 'image': <a PIL image object for the frame>\n",
    "            }\n",
    "            `write_images`, if true, causes the extracted frame images to be written to a folder\n",
    "            specified by `folder_name`, with the naming scheme `image00001.png`\n",
    "        \"\"\"\n",
    "        \n",
    "        GC_INTERVAL = 1000\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print('total frames in video:',total_frames)\n",
    "\n",
    "        video_framerate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        print('video FPS:',video_framerate)\n",
    "        frame_duration = 1 / float(video_framerate)\n",
    "\n",
    "        frame_count = 0.0\n",
    "        frames_processed = 0\n",
    "        timecode = 0.0\n",
    "        skip_until = start_seconds\n",
    "\n",
    "        pose_output = []\n",
    "\n",
    "        if write_images:\n",
    "            if not os.path.isdir(folder_name):\n",
    "                os.mkdir(folder_name)\n",
    "            for filename in os.listdir(folder_name):\n",
    "                file_path = os.path.join(folder_name, filename)\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "\n",
    "        while cap.isOpened() and (frame_count < total_frames):\n",
    "            ret_val, im = cap.read()\n",
    "\n",
    "            timecode = frame_count * frame_duration\n",
    "            frame_count += 1\n",
    "\n",
    "            if (end_seconds and timecode > end_seconds) or (max_frames and frames_processed >= max_frames):\n",
    "                return pose_output\n",
    "\n",
    "            if timecode < start_seconds:\n",
    "                continue\n",
    "\n",
    "            if im is None:\n",
    "                # Might want to retry here\n",
    "                # print(\"Missed a frame, continuing...\")\n",
    "                # For now, we'll count a missed frame as a processed frame\n",
    "                continue\n",
    "\n",
    "            if seconds_to_skip and timecode < skip_until:\n",
    "                continue\n",
    "            else:\n",
    "                skip_until += seconds_to_skip\n",
    "\n",
    "            im_height, im_width, im_channels = im.shape\n",
    "\n",
    "            frame_id = int(round(cap.get(1)))\n",
    "\n",
    "            # Image doesn't necessarily come in as RGB(A)!\n",
    "            rgbim = cv2.cvtColor(im, cv2.COLOR_BGR2RGBA)\n",
    "            pil_image = PIL.Image.fromarray(rgbim)\n",
    "\n",
    "            detections = self.__detect_pil_image__(pil_image)\n",
    "\n",
    "            print(\"Frame\",frame_count,\"of\",total_frames,round(timecode,2),\"figures\",len(detections))\n",
    "\n",
    "            this_frame_data = {'frame_id': frame_count, 'time': timecode, 'figures': detections} #, 'flipped_figures': flipped_detections, 'zeroified_figures': zeroified_detections}\n",
    "            if images_too:\n",
    "                this_frame_data['image'] = rgbim\n",
    "            if write_images:\n",
    "                savepath = os.path.join(folder_name, 'image' + str(int(frames_processed + 1)).zfill(5) + '.png')\n",
    "                self.overlay_poses(im, this_frame_data, source_figure='figures', savepath=savepath)\n",
    "                del im, rgbim, pil_image\n",
    "\n",
    "            pose_output.append(this_frame_data)\n",
    "            frames_processed += 1\n",
    "\n",
    "        return pose_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teddy = Detector()\n",
    "detections = teddy.detect_image('sample_data/sample2.png', viz=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teddy = Detector()\n",
    "pose_output = teddy.detect_video(\"/srv/choreo/Einstein.mp4\", start_seconds=6310, end_seconds=7288, write_images=True, images_too=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 9c33b2f Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/home/broadwell/anaconda3/envs/torch --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "Input #0, image2, from 'video_folder/*.png':\n",
      "  Duration: 00:16:18.04, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: png, rgba(pc), 2000x1200 [SAR 7874:7874 DAR 5:3], 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mprofile High, level 5.0, 4:2:0, 8-bit\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0m264 - core 161 r3030M 8bd6d28 - H.264/MPEG-4 AVC codec - Copyleft 2003-2020 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=30 lookahead_threads=5 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'Einstein_ballet_pf.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 2000x1200 [SAR 1:1 DAR 5:3], q=-1--1, 25 fps, 12800 tbn, 25 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.91.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=24451 fps= 90 q=-1.0 Lsize=  191601kB time=00:16:17.92 bitrate=1605.0kbits/s speed=3.61x    \n",
      "video:191317kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.148833%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mframe I:186   Avg QP:16.40  size: 36308\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mframe P:7025  Avg QP:19.08  size: 13608\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mframe B:17240 Avg QP:21.23  size:  5427\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mconsecutive B-frames:  3.3%  6.3%  5.6% 84.8%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mmb I  I16..4: 33.6% 58.5%  7.9%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mmb P  I16..4:  6.1%  6.3%  1.1%  P16..4: 15.9%  2.7%  1.3%  0.0%  0.0%    skip:66.6%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mmb B  I16..4:  0.5%  0.6%  0.2%  B16..8: 13.0%  1.6%  0.3%  direct: 1.0%  skip:82.7%  L0:44.7% L1:53.1% BI: 2.2%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0m8x8 transform intra:48.4% inter:87.2%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mcoded y,uvDC,uvAC intra: 22.1% 39.9% 13.4% inter: 2.4% 6.0% 0.5%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mi16 v,h,dc,p: 18% 60%  3% 19%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 19% 30% 33%  2%  3%  4%  4%  3%  3%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 34% 22% 17%  4%  5%  6%  4%  5%  3%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mi8c dc,h,v,p: 54% 27% 16%  3%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mref P L0: 62.6%  3.7% 22.6% 11.1%  0.0%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mref B L0: 82.0% 14.5%  3.5%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mref B L1: 94.9%  5.1%\n",
      "\u001b[1;36m[libx264 @ 0x55ae6ea67340] \u001b[0mkb/s:1602.45\n"
     ]
    }
   ],
   "source": [
    "FPS = 25\n",
    "!ffmpeg -y -framerate $FPS -pattern_type glob -i 'video_folder/*.png' -strict '-2' -c:v libx264 -vf \"fps=$FPS\" -pix_fmt yuv420p Einstein_ballet_pf.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    teddy = Detector()\n",
    "    detections = teddy.detect_image('sample_data/sample1.png')\n",
    "    print(detections[0])\n",
    "except:\n",
    "    print(\"Unable to instantiate a detector on your system. Do you have PyTorch with CUDA enabled?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Detector.detect_image)\n",
    "show_doc(Detector.detect_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
