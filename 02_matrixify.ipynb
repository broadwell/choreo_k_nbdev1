{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Tools to convert detected/corrected poses to matrix representations\n",
    "output-file: matrixify.html\n",
    "title: matrixify\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp matrixify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%pip install --upgrade scikit-bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.spatial.distance import pdist, cosine\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import lil_matrix\n",
    "import networkx as nx\n",
    "from choreo_k.modify import flip_detections, flip_detections_y_first\n",
    "\n",
    "\n",
    "def matrixify_pose(coords_and_confidence):\n",
    "    \"\"\" DISTANCE MATRIX: compute a pose's L1-normed inter-keypoint distance matrix.\n",
    "        To compare any two poses, we can measure the degree of correlation between\n",
    "        their distance matrices via a statistical test, such as the Mantel test.\n",
    "        XXX It's not obvious that normalizing the matrix really makes a difference to\n",
    "        the final correlation comparison, but it doesn't seem to hurt, either...\n",
    "        Note that if the pose representation has 17 keypoints, then each pose instance\n",
    "        can be represented by a condensed distance matrix (or vector) of 136 elements.\n",
    "    \"\"\"\n",
    "    \n",
    "    if coords_and_confidence.shape[0] == 0:\n",
    "            return None\n",
    "    coords = coords_and_confidence[:,:2]\n",
    "    condensed_distance_matrix = normalize(pdist(coords, 'sqeuclidean').reshape(1, -1))[0,:]\n",
    "    return condensed_distance_matrix\n",
    "\n",
    "\n",
    "def get_normalized_coords(frame, figure_index=0, figure_type='figures', norm='l2'):\n",
    "    if figure_type not in frame or figure_index > len(frame[figure_type])-1:\n",
    "        return None\n",
    "    coords_and_confidence = frame[figure_type][figure_index].data\n",
    "    if coords_and_confidence.shape[0] == 0:\n",
    "        return None\n",
    "    coords = np.copy(coords_and_confidence)\n",
    "\n",
    "    normalized_coords = normalize(coords_and_confidence[:,:2], norm=norm, axis=0)\n",
    "    coords[:,:2] = normalized_coords[:,:2]\n",
    "    return coords\n",
    "\n",
    "# An extension of get_normalized_pose to deal with the need to flip some poses vertically\n",
    "# and/or horizontally\n",
    "def normalize_pose(frame, figure_index=0, figure_type='figures', norm='l2', y_first=True, flip_x=False, flip_y=False, mirror_coco_17_left_right=False):\n",
    "    if figure_type not in frame or figure_index > len(frame[figure_type])-1:\n",
    "        return None\n",
    "    # For some pose estimation libraries, such as TF MoveNet, the coords and confidence values\n",
    "    # of the detected armature points are in YXC format, rather than XYC (ugh).\n",
    "    normalized_frame = copy.deepcopy(frame)\n",
    "    if flip_x or flip_y:\n",
    "        if y_first or ('y_first' in frame and frame['y_first']):\n",
    "            flipped_detections = flip_detections_y_first(frame[figure_type], flip_x=flip_x, flip_y=flip_y, mirror_coco_17_left_right=mirror_coco_17_left_right)\n",
    "        else:\n",
    "            flipped_detections = flip_detections(frame[figure_type], flip_x=flip_x, flip_y=flip_y, mirror_coco_17_left_right=mirror_coco_17_left_right)\n",
    "        normalized_frame[figure_type] = flipped_detections\n",
    "    return get_normalized_coords(normalized_frame, norm=norm)\n",
    "\n",
    "def symmetrify_pose(frame, figure_index=0, figure_type='figures', y_first=True):\n",
    "    if figure_type not in frame or figure_index > len(frame[figure_type])-1:\n",
    "        return None\n",
    "    # For some pose estimation libraries, such as TF MoveNet, the coords and confidence values\n",
    "    # of the detected armature points are in YXC format, rather than XYC (ugh).\n",
    "    flipped_frame = copy.deepcopy(frame)\n",
    "    if y_first or ('y_first' in frame and frame['y_first']):\n",
    "        flipped_detections = flip_detections_y_first(frame[figure_type], flip_y=False, flip_x=True)\n",
    "    else:\n",
    "        flipped_detections = flip_detections(frame[figure_type], flip_y=False, flip_x=True)\n",
    "\n",
    "    flipped_frame[figure_type] = flipped_detections\n",
    "    normalized_coords = get_normalized_coords(frame)\n",
    "    normalized_flipped_coords = get_normalized_coords(flipped_frame)\n",
    "    orig_coords_count = normalized_coords.shape[0]\n",
    "    if orig_coords_count == 0:\n",
    "        return None\n",
    "    sym_coords_and_confidence = np.zeros((orig_coords_count*2,3))\n",
    "    sym_coords_and_confidence[:orig_coords_count,:] = normalized_coords\n",
    "    sym_coords_and_confidence[orig_coords_count:,:] = normalized_flipped_coords\n",
    "    return sym_coords_and_confidence\n",
    "\n",
    "def normalize_symmetrify_and_compare_poses_cosine(p1, p2):\n",
    "    p1_symmetrized = symmetrify_pose(p1)\n",
    "    p2_symmetrized = symmetrify_pose(p2)\n",
    "    if p1_symmetrized is None or p2_symmetrized is None:\n",
    "        return 0 # No similarity if one or both poses is missing\n",
    "    return compare_poses_cosine(p1_symmetrized, p2_symmetrized)   \n",
    "\n",
    "\n",
    "def normalize_and_compare_poses_cosine(p1, p2): # Uses cosine distance\n",
    "    p1_normalized = get_normalized_coords(p1)\n",
    "    p2_normalized = get_normalized_coords(p2)\n",
    "    if p1_normalized is None or p2_normalized is None:\n",
    "        return 0 # No similarity if one or both poses is missing\n",
    "    return compare_poses_cosine(p1_normalized, p2_normalized)\n",
    "\n",
    "\n",
    "def compare_poses_cosine(p1, p2):\n",
    "    return 1 - cosine(p1[:,:2].flatten(), p2[:,:2].flatten())\n",
    "\n",
    "\n",
    "def get_pose_matrix(frame, figure_index=0, figure_type='flipped_figures'):\n",
    "    if figure_type not in frame or figure_index > len(frame[figure_type])-1:\n",
    "        return None\n",
    "    coords_and_confidence = frame[figure_type][figure_index].data\n",
    "    return matrixify_pose(coords_and_confidence)\n",
    "\n",
    "\n",
    "def get_laplacian_matrix(frame, normalized=True, show=False, figure_index=0, figure_type='flipped_figures'):\n",
    "    \"\"\" LAPLACIAN: compute the Delaunay triangulation between keypoints, then\n",
    "        use the connections to build an adjacency matrix, which is then converted\n",
    "        to its (normalized) Laplacian matrix (a single matrix that encapsulates the\n",
    "        degree of each node and the connections between the nodes). Then you can\n",
    "        subtract a pose's Laplacian from another's to get a measure of the degree of\n",
    "        similarity or difference between them.\n",
    "    \"\"\"\n",
    "    \n",
    "    if figure_type not in frame or figure_index > len(frame[figure_type])-1 or frame[figure_type][figure_index].data.shape[0] == 0:\n",
    "        return None\n",
    "    \n",
    "    all_points = frame[figure_type][figure_index].data\n",
    "\n",
    "    # For visualization, remove all [x,y,0] (unknown) coordinates.\n",
    "    nonzero = (all_points!=0).all(axis=1)\n",
    "    nz_points = all_points[nonzero]\n",
    "    points = nz_points[:,:2]\n",
    "    total_points = len(points)\n",
    "    try:\n",
    "        tri = Delaunay(points)\n",
    "    except:\n",
    "        # Not sure why this happens -- maybe the points are all in a line or something\n",
    "        print(\"Error computing Delaunay triangulation\")\n",
    "        return None\n",
    "\n",
    "    if show:\n",
    "        plot_delaunay(frame[figure_type][figure_index])\n",
    "\n",
    "    adjacency_matrix = lil_matrix((total_points, total_points), dtype=int)\n",
    "    for i in np.arange(0, np.shape(tri.simplices)[0]):\n",
    "        for j in tri.simplices[i]:\n",
    "            if j < total_points:\n",
    "                adjacency_matrix[j, tri.simplices[i][tri.simplices[i] < total_points]] = 1\n",
    "\n",
    "    adjacency_graph = nx.from_scipy_sparse_matrix(adjacency_matrix)\n",
    "\n",
    "    if normalized:\n",
    "        lm = nx.normalized_laplacian_matrix(adjacency_graph)\n",
    "    else:\n",
    "        lm = nx.laplacian_matrix(adjacency_graph)\n",
    "\n",
    "    return lm\n",
    "\n",
    "\n",
    "def compare_laplacians(p1, p2, figure_index=0, figure_type='flipped_figures', show=False):\n",
    "    lm1 = get_laplacian_matrix(p1, figure_index=figure_index, figure_type=figure_type, show=show).todense()\n",
    "    lm2 = get_laplacian_matrix(p2, figure_index=figure_index, figure_type=figure_type, show=show).todense()\n",
    "    if lm1.shape[0] != lm2.shape[0]:\n",
    "        return None\n",
    "    movement = np.subtract(lm1, lm2)\n",
    "    return 1 - abs(movement.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
