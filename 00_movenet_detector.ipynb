{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp movenet_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detect\n",
    "\n",
    "> Pose Detector class based on Open PifPaf and some pose modification tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#%pip install tensorflow\n",
    "#%pip install tensorflow-hub\n",
    "#%pip install opencv-python\n",
    "#%pip install git+https://github.com/tensorflow/docs\n",
    "#%pip install tensorflow-io\n",
    "#%pip install imageio\n",
    "\n",
    "#%pip install wget\n",
    "#import wget\n",
    "#wget.download(\"https://tfhub.dev/google/movenet/singlepose/thunder/4?tf-hub-format=compressed\")\n",
    "#!tar -zxf movenet_singlepose_thunder_4.tar.gz --directory movenet_singlepose_thunder_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "# Import matplotlib libraries\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "#from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow_docs.vis import embed\n",
    "\n",
    "from io import BytesIO\n",
    "import PIL\n",
    "from IPython.display import display, Image, clear_output\n",
    "\n",
    "\n",
    "def display_img_array(ima):\n",
    "    im = PIL.Image.fromarray(ima)\n",
    "    bio = BytesIO()\n",
    "    im.save(bio, format='png')\n",
    "    display(Image(bio.getvalue(), format='png'))\n",
    "\n",
    "# See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/pose_classification.ipynb\n",
    "# Also https://www.tensorflow.org/hub/tutorials/movenet\n",
    "\n",
    "class Detection:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "        \n",
    "class Detector:\n",
    "    \"\"\"Given a still image (or video frame), finds poses.\n",
    "    \n",
    "    Attributes:  \n",
    "        module: The pose detection module being used for inference\n",
    "        input_size: The input resolution of the model\n",
    "        model_names: Shorthand dict of pose detection models that should be available\n",
    "                     locally, with their local folder name and download URLs as values\n",
    "    \"\"\"\n",
    "    \n",
    "    model_names = { \"movenet_singlepose_thunder\": [\"movenet_singlepose_thunder_4\", \"https://tfhub.dev/google/movenet/singlepose/thunder/4\"],\n",
    "                    \"movenet_multipose_lightning\": [\"movenet_multipose_lightning_1\", \"https://tfhub.dev/google/movenet/multipose/lightning/1\"]\n",
    "                  }\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_size = 256\n",
    "        print(\"Models available for loading via init_model(model_name=):\\n\" + \"\\n\".join(list(self.model_names.keys())))\n",
    "        \n",
    "        \n",
    "    def init_model(self, model_url=None, model_name=\"movenet_singlepose_thunder\"):\n",
    "        if model_url is not None:\n",
    "            print(\"Loading model at\", model_url)\n",
    "            self.module = hub.load(model_url)\n",
    "        else:\n",
    "            if Path(self.model_names[model_name][0]).exists():\n",
    "                print(\"Loading\", model_name, \"from\", self.model_names[model_name][0] + \"/\")\n",
    "                self.module = tf.saved_model.load(self.model_names[model_name][0])\n",
    "            else:\n",
    "                print(\"Downloading\", model_name, \"from\", self.model_names[model_name][1])\n",
    "                self.module = hub.load(self.model_names[model_name][1])\n",
    "\n",
    "                \n",
    "    # Dictionary that maps from joint names to keypoint indices.\n",
    "    KEYPOINT_DICT = {\n",
    "        'nose': 0,\n",
    "        'left_eye': 1,\n",
    "        'right_eye': 2,\n",
    "        'left_ear': 3,\n",
    "        'right_ear': 4,\n",
    "        'left_shoulder': 5,\n",
    "        'right_shoulder': 6,\n",
    "        'left_elbow': 7,\n",
    "        'right_elbow': 8,\n",
    "        'left_wrist': 9,\n",
    "        'right_wrist': 10,\n",
    "        'left_hip': 11,\n",
    "        'right_hip': 12,\n",
    "        'left_knee': 13,\n",
    "        'right_knee': 14,\n",
    "        'left_ankle': 15,\n",
    "        'right_ankle': 16\n",
    "    }\n",
    "\n",
    "    # Maps bones to a matplotlib color name.\n",
    "    KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "        (0, 1): 'm',\n",
    "        (0, 2): 'c',\n",
    "        (1, 3): 'm',\n",
    "        (2, 4): 'c',\n",
    "        (0, 5): 'm',\n",
    "        (0, 6): 'c',\n",
    "        (5, 7): 'm',\n",
    "        (7, 9): 'm',\n",
    "        (6, 8): 'c',\n",
    "        (8, 10): 'c',\n",
    "        (5, 6): 'y',\n",
    "        (5, 11): 'm',\n",
    "        (6, 12): 'c',\n",
    "        (11, 12): 'y',\n",
    "        (11, 13): 'm',\n",
    "        (13, 15): 'm',\n",
    "        (12, 14): 'c',\n",
    "        (14, 16): 'c'\n",
    "    }\n",
    "\n",
    "    # Confidence score to determine whether a keypoint prediction is reliable.\n",
    "    MIN_CROP_KEYPOINT_SCORE = 0.2\n",
    "\n",
    "    \n",
    "    def __keypoints_and_edges_for_display__(self, keypoints_with_scores, height, width, height_crop_ratio=1.0, width_crop_ratio=1.0, height_offset=0, width_offset=0, keypoint_threshold=0.0):\n",
    "        \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "        Args:\n",
    "          keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "            the keypoint coordinates and scores returned from the MoveNet model.\n",
    "          height: height of the image in pixels.\n",
    "          width: width of the image in pixels.\n",
    "          height_crop_ratio: If the height of the display overlay image is different\n",
    "            from the version of the image used as input to the pose detection model,\n",
    "            this is used to scale the keypoint positions (in concert with the\n",
    "            height_offset parameter -- see below) so that they appear over the\n",
    "            appropriate positions on the display image.\n",
    "          width_crop_ratio: See height_crop_ratio above.\n",
    "          height_offset: If the height of the display overlay image is different from\n",
    "            the version of the image used as input to the pose detection model, this\n",
    "            is used to shift the keypoint positions (in concert with the\n",
    "            height_crop_ratio parameter -- see above) so that they appear over the\n",
    "            appropriate positions on the display image.\n",
    "          width_offset: See height_offset above.\n",
    "          keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "            visualized.\n",
    "\n",
    "        Returns:\n",
    "          A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "            * the coordinates of all keypoints of all detected entities;\n",
    "            * the coordinates of all skeleton edges of all detected entities;\n",
    "            * the colors in which the edges should be plotted.\n",
    "        \"\"\"\n",
    "        keypoints_all = []\n",
    "        keypoint_edges_all = []\n",
    "        edge_colors = []\n",
    "\n",
    "        if (len(keypoints_with_scores[0][0]) > 0):\n",
    "            num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "            for idx in range(num_instances):\n",
    "                kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "                kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "                kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "                kpts_absolute_xy = np.stack(\n",
    "                    [(width * np.array(kpts_x) * width_crop_ratio) + width_offset, (height * np.array(kpts_y) * height_crop_ratio) - height_offset], axis=-1)\n",
    "                kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "                    kpts_scores > keypoint_threshold, :]\n",
    "                keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "                for edge_pair, color in self.KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "                    if (kpts_scores[edge_pair[0]] > keypoint_threshold and kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "                        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "                        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "                        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "                        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "                        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "                        keypoint_edges_all.append(line_seg)\n",
    "                        edge_colors.append(color)\n",
    "        if keypoints_all:\n",
    "            keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "        else:\n",
    "            keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "        if keypoint_edges_all:\n",
    "            edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "        else:\n",
    "            edges_xy = np.zeros((0, 2, 2))\n",
    "        return keypoints_xy, edges_xy, edge_colors\n",
    "    \n",
    "    \n",
    "    def __unitize_keypoints__(self, keypoint_scores_array, image_height, image_width):\n",
    "        unitized_keypoint_scores = np.copy(keypoint_scores_array)\n",
    "        \n",
    "        if (len(unitized_keypoint_scores) > 0):\n",
    "            for idx in range(17):\n",
    "                unitized_keypoint_scores[idx, 0] = keypoint_scores_array[idx, 0] / image_height\n",
    "                unitized_keypoint_scores[idx, 1] = keypoint_scores_array[idx, 1] / image_width\n",
    "        \n",
    "        return unitized_keypoint_scores\n",
    "    \n",
    "    \n",
    "    def draw_predictions_on_image(self, image, detections, pose_confidence_scores=[], blank_background=False):\n",
    "        \n",
    "        im = self.__decode_image__(image).numpy()\n",
    "        \n",
    "        image_height, image_width, _ = im.shape\n",
    "        \n",
    "        unitized_detections = [self.__unitize_keypoints__(detection.data, image_height, image_width) for detection in detections]\n",
    "        \n",
    "        return self.__draw_predictions_on_image__(im, unitized_detections, pose_confidence_scores, blank_background=blank_background)\n",
    "\n",
    "    \n",
    "    def __draw_predictions_on_image__(self, image, detections, pose_confidence_scores, crop_region=None, output_image_height=None, output_image_width=None, blank_background=False, confidence_threshold=0.1):\n",
    "        \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "        Args:\n",
    "            image: A numpy array with shape [height, width, channel] representing the\n",
    "            pixel values of the input image.\n",
    "            detections: An array of numpy arrays of shape [17, 3] representing\n",
    "            the keypoint coordinates and scores returned from the MoveNet model.\n",
    "            pose_confidence_scores: confidence score (0 to 1) of every detection.\n",
    "            crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "            of the crop region in normalized coordinates (see the init_crop_region\n",
    "            function below for more detail). If provided, this function will also\n",
    "            draw the bounding box on the image.\n",
    "            output_image_height: The height in pixels of the output image. If specified,\n",
    "            the image will be cropped to fit the desired output height and width, with\n",
    "            the positions of the visualized keypoints adjusted as necessary.\n",
    "            output_image_width: The width in pexels of the output image. If this is not\n",
    "            specified but output_image_height is, its value will be inferred to\n",
    "            conform with the aspect ratio of the input image (see output_image_height).\n",
    "\n",
    "        Returns:\n",
    "            A numpy array with shape [out_height, out_width, channel] representing the\n",
    "            image overlaid with keypoint predictions.\n",
    "        \"\"\"\n",
    "        height, width, _ = image.shape\n",
    "        aspect_ratio = float(width) / height\n",
    "\n",
    "        height_crop_ratio = 1\n",
    "        width_crop_ratio = 1\n",
    "        height_offset = 0\n",
    "        width_offset = 0\n",
    "\n",
    "        if output_image_height is not None:\n",
    "            if output_image_width is None:\n",
    "                output_image_width = int(output_image_height * aspect_ratio)\n",
    "\n",
    "            height_offset = int(height / 2) - int(output_image_height / 2)\n",
    "            width_offset = int(width / 2) - int(output_image_width / 2)\n",
    "\n",
    "            height_crop_ratio = float(height) / output_image_height\n",
    "            width_crop_ratio = float(width) / output_image_width\n",
    "\n",
    "            crop_y = max(0, height_offset)\n",
    "            crop_x = max(0, width_offset)\n",
    "\n",
    "            #output_image_width = int(output_image_height / height * width)\n",
    "            # image_from_plot = cv2.resize(\n",
    "            #     image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "            #     interpolation=cv2.INTER_CUBIC)\n",
    "            image = image[crop_y:min(height, crop_y+output_image_height), crop_x:min(width, crop_x+output_image_width)]\n",
    "            height, width, _ = image.shape\n",
    "            aspect_ratio = float(width) / height  \n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "        # To remove the huge white borders\n",
    "        fig.tight_layout(pad=0)\n",
    "        ax.margins(0)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        plt.axis('off')\n",
    "\n",
    "        if blank_background:\n",
    "            image = np.zeros([height,width,3],dtype=np.uint8)\n",
    "            image.fill(255)\n",
    "\n",
    "        im = ax.imshow(image)\n",
    "        \n",
    "        for f, keypoint_scores_array in enumerate(detections):\n",
    "            \n",
    "            if len(pose_confidence_scores) > f and pose_confidence_scores[f] > confidence_threshold:\n",
    "                print(\"Drawing figure with confidence score\",pose_confidence_scores[f])\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "            ax.add_collection(line_segments)\n",
    "            # Turn off tick labels\n",
    "            scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "        \n",
    "            keypoints_with_scores = tf.expand_dims(tf.expand_dims(keypoint_scores_array, axis=0), axis=0)\n",
    "            \n",
    "            (keypoint_locs, keypoint_edges,\n",
    "            edge_colors) = self.__keypoints_and_edges_for_display__(\n",
    "                keypoints_with_scores, height, width, height_crop_ratio, width_crop_ratio, height_offset, width_offset)\n",
    "\n",
    "            line_segments.set_segments(keypoint_edges)\n",
    "            line_segments.set_color(edge_colors)\n",
    "            if keypoint_edges.shape[0]:\n",
    "                line_segments.set_segments(keypoint_edges)\n",
    "                line_segments.set_color(edge_colors)\n",
    "            if keypoint_locs.shape[0]:\n",
    "                scat.set_offsets(keypoint_locs)\n",
    "\n",
    "        if crop_region is not None:\n",
    "            xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "            ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "            rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "            rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "            rect = patches.Rectangle(\n",
    "                (xmin,ymin),rec_width,rec_height,\n",
    "                linewidth=1,edgecolor='b',facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        image_from_plot = image_from_plot.reshape(\n",
    "            fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "\n",
    "        return image_from_plot\n",
    "\n",
    "    \n",
    "    def __to_gif__(self, images, fps):\n",
    "        \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
    "        imageio.mimsave('./animation.gif', images, fps=fps)\n",
    "        return embed.embed_file('./animation.gif')\n",
    "    \n",
    "    \n",
    "    def gif_from_detections(self, pose_output, fps=25, blank_background=False):\n",
    "        output_images = []\n",
    "        bar = display(self.__progress__(0, len(pose_output)-1), display_id=True)\n",
    "        for frameno, frame_data in enumerate(pose_output):\n",
    "            bar.update(self.__progress__(frameno, len(pose_output)-1))\n",
    "            output_images.append(self.draw_predictions_on_image(frame_data['image'], frame_data['figures'], blank_background=blank_background))\n",
    "        output = np.stack(output_images, axis=0)\n",
    "        self.__to_gif__(output, fps=fps)\n",
    "\n",
    "        \n",
    "    def __progress__(self, value, max=100):\n",
    "        return HTML(\"\"\"\n",
    "            <progress\n",
    "                value='{value}'\n",
    "                max='{max}',\n",
    "                style='width: 100%'\n",
    "            >\n",
    "                {value}\n",
    "            </progress>\n",
    "        \"\"\".format(value=value, max=max))\n",
    "\n",
    "    \n",
    "    def __init_crop_region__(self, image_height, image_width):\n",
    "        \"\"\"Defines the default crop region.\n",
    "\n",
    "        The function provides the initial crop region (pads the full image from both\n",
    "        sides to make it a square image) when the algorithm cannot reliably determine\n",
    "        the crop region from the previous frame.\n",
    "        \"\"\"\n",
    "        if image_width > image_height:\n",
    "            box_height = image_width / image_height\n",
    "            box_width = 1.0\n",
    "            y_min = (image_height / 2 - image_width / 2) / image_height\n",
    "            x_min = 0.0\n",
    "        else:\n",
    "            box_height = 1.0\n",
    "            box_width = image_height / image_width\n",
    "            y_min = 0.0\n",
    "            x_min = (image_width / 2 - image_height / 2) / image_width\n",
    "\n",
    "        return {\n",
    "            'y_min': y_min,\n",
    "            'x_min': x_min,\n",
    "            'y_max': y_min + box_height,\n",
    "            'x_max': x_min + box_width,\n",
    "            'height': box_height,\n",
    "            'width': box_width\n",
    "        }\n",
    "\n",
    "    \n",
    "    def __torso_visible__(self, keypoints):\n",
    "        \"\"\"Checks whether there are enough torso keypoints.\n",
    "\n",
    "        This function checks whether the model is confident at predicting one of the\n",
    "        shoulders/hips which is required to determine a good crop region.\n",
    "        \"\"\"\n",
    "        return ((keypoints[0, 0, self.KEYPOINT_DICT['left_hip'], 2] >\n",
    "                self.MIN_CROP_KEYPOINT_SCORE or\n",
    "                keypoints[0, 0, self.KEYPOINT_DICT['right_hip'], 2] >\n",
    "                self.MIN_CROP_KEYPOINT_SCORE) and\n",
    "                (keypoints[0, 0, self.KEYPOINT_DICT['left_shoulder'], 2] >\n",
    "                self.MIN_CROP_KEYPOINT_SCORE or\n",
    "                keypoints[0, 0, self.KEYPOINT_DICT['right_shoulder'], 2] >\n",
    "                self.MIN_CROP_KEYPOINT_SCORE))\n",
    "\n",
    "    \n",
    "    def __determine_torso_and_body_range__(self, keypoints, target_keypoints, center_y, center_x):\n",
    "        \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
    "\n",
    "        The function returns the maximum distances from the two sets of keypoints:\n",
    "        full 17 keypoints and 4 torso keypoints. The returned information will be\n",
    "        used to determine the crop size. See determineCropRegion for more detail.\n",
    "        \"\"\"\n",
    "        torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "        max_torso_yrange = 0.0\n",
    "        max_torso_xrange = 0.0\n",
    "        for joint in torso_joints:\n",
    "            dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "            dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "            if dist_y > max_torso_yrange:\n",
    "                max_torso_yrange = dist_y\n",
    "            if dist_x > max_torso_xrange:\n",
    "                max_torso_xrange = dist_x\n",
    "\n",
    "        max_body_yrange = 0.0\n",
    "        max_body_xrange = 0.0\n",
    "        for joint in self.KEYPOINT_DICT.keys():\n",
    "            if keypoints[0, 0, self.KEYPOINT_DICT[joint], 2] < self.MIN_CROP_KEYPOINT_SCORE:\n",
    "                continue\n",
    "            dist_y = abs(center_y - target_keypoints[joint][0]);\n",
    "            dist_x = abs(center_x - target_keypoints[joint][1]);\n",
    "            if dist_y > max_body_yrange:\n",
    "                max_body_yrange = dist_y\n",
    "\n",
    "            if dist_x > max_body_xrange:\n",
    "                max_body_xrange = dist_x\n",
    "\n",
    "        return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
    "\n",
    "    \n",
    "    def __determine_crop_region__(self, keypoints, image_height, image_width):\n",
    "        \"\"\"Determines the region to crop the image for the model to run inference on.\n",
    "\n",
    "        The algorithm uses the detected joints from the previous frame to estimate\n",
    "        the square region that encloses the full body of the target person and\n",
    "        centers at the midpoint of two hip joints. The crop size is determined by\n",
    "        the distances between each joints and the center point.\n",
    "        When the model is not confident with the four torso joint predictions, the\n",
    "        function returns a default crop which is the full image padded to square.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Don't bother computing a crop region for a multi-pose image\n",
    "        if keypoints.shape[1] != 1:\n",
    "            return self.__init_crop_region__(image_height, image_width)\n",
    "        \n",
    "        target_keypoints = {}\n",
    "        for joint in self.KEYPOINT_DICT.keys():\n",
    "            target_keypoints[joint] = [\n",
    "            keypoints[0, 0, self.KEYPOINT_DICT[joint], 0] * image_height,\n",
    "            keypoints[0, 0, self.KEYPOINT_DICT[joint], 1] * image_width\n",
    "            ]\n",
    "\n",
    "        if self.__torso_visible__(keypoints):\n",
    "            center_y = (target_keypoints['left_hip'][0] +\n",
    "                        target_keypoints['right_hip'][0]) / 2;\n",
    "            center_x = (target_keypoints['left_hip'][1] +\n",
    "                        target_keypoints['right_hip'][1]) / 2;\n",
    "\n",
    "            (max_torso_yrange, max_torso_xrange,\n",
    "            max_body_yrange, max_body_xrange) = self.__determine_torso_and_body_range__(\n",
    "                keypoints, target_keypoints, center_y, center_x)\n",
    "\n",
    "            crop_length_half = np.amax(\n",
    "                [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
    "                max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
    "\n",
    "            tmp = np.array(\n",
    "                [center_x, image_width - center_x, center_y, image_height - center_y])\n",
    "            crop_length_half = np.amin(\n",
    "                [crop_length_half, np.amax(tmp)]);\n",
    "\n",
    "            crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
    "\n",
    "            if crop_length_half > max(image_width, image_height) / 2:\n",
    "                return self.__init_crop_region__(image_height, image_width)\n",
    "            else:\n",
    "                crop_length = crop_length_half * 2;\n",
    "            return {\n",
    "                'y_min': crop_corner[0] / image_height,\n",
    "                'x_min': crop_corner[1] / image_width,\n",
    "                'y_max': (crop_corner[0] + crop_length) / image_height,\n",
    "                'x_max': (crop_corner[1] + crop_length) / image_width,\n",
    "                'height': (crop_corner[0] + crop_length) / image_height -\n",
    "                    crop_corner[0] / image_height,\n",
    "                'width': (crop_corner[1] + crop_length) / image_width -\n",
    "                    crop_corner[1] / image_width\n",
    "            }\n",
    "        else:\n",
    "            return self.__init_crop_region__(image_height, image_width)\n",
    "\n",
    "        \n",
    "    def __crop_and_resize__(self, image, crop_region, crop_size):\n",
    "        \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
    "        boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
    "                crop_region['y_max'], crop_region['x_max']]]\n",
    "        output_image = tf.image.crop_and_resize(\n",
    "            image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
    "        return output_image\n",
    "\n",
    "    \n",
    "    def __run_inference__(self, image, crop_region, crop_size):\n",
    "        \"\"\"Runs model inference on the cropped region.\n",
    "\n",
    "        The function runs the model inference on the cropped region and updates the\n",
    "        model output to the original image coordinate system.\n",
    "        \"\"\"\n",
    "        image_height, image_width, _ = image.shape\n",
    "        input_image = self.__crop_and_resize__(\n",
    "            tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
    "        # Run model inference.\n",
    "        inference_output = self.__detect__(input_image)\n",
    "        \n",
    "        pose_confidence_scores = []\n",
    "\n",
    "        if inference_output.shape[1] == 1:\n",
    "            output_keypoints_with_scores = np.copy(inference_output)\n",
    "            pose_confidence_scores.append(1.0)\n",
    "        # Handle multi-pose output -- need to expand each 56-element [y1,x1,score1,y2,x2,score2,...]\n",
    "        # sub-array to a (17, 3) [[y1, x1, score1], [y2, x2, score2], ...] sub-array\n",
    "        # Note that the final 5 elements of each per-pose subarray in the multi-pose output are\n",
    "        # ymin, xmin, ymax, xmax, pose_score.\n",
    "        else:\n",
    "            output_keypoints_with_scores = np.zeros((1, inference_output.shape[1], 17, 3))\n",
    "            for idx in range(inference_output.shape[1]):\n",
    "                for kpt in range(17):\n",
    "                    output_keypoints_with_scores[0, idx, kpt, 0] = inference_output[0, idx, kpt*3]\n",
    "                    output_keypoints_with_scores[0, idx, kpt, 1] = inference_output[0, idx, kpt*3 + 1]\n",
    "                    output_keypoints_with_scores[0, idx, kpt, 2] = inference_output[0, idx, kpt*3 + 2]\n",
    "                pose_confidence_scores.append(inference_output[0, idx, 55])\n",
    "        \n",
    "        # Update the coordinates.\n",
    "        for idx in range(output_keypoints_with_scores.shape[1]):\n",
    "            for kpt in range(17):\n",
    "                output_keypoints_with_scores[0, idx, kpt, 0] = (\n",
    "                    crop_region['y_min'] * image_height +\n",
    "                    crop_region['height'] * image_height *\n",
    "                    output_keypoints_with_scores[0, idx, kpt, 0]) #/ image_height\n",
    "                output_keypoints_with_scores[0, idx, kpt, 1] = (\n",
    "                    crop_region['x_min'] * image_width +\n",
    "                    crop_region['width'] * image_width *\n",
    "                    output_keypoints_with_scores[0, idx, kpt, 1]) #/ image_width\n",
    "\n",
    "        return [output_keypoints_with_scores, pose_confidence_scores]\n",
    "\n",
    "                                              \n",
    "    def __decode_image_file__(self, image_path):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        if Path(image_path).suffix == \".jpeg\":\n",
    "            return tf.image.decode_jpeg(image)\n",
    "        elif Path(image_path).suffix == \".png\":\n",
    "            return tf.image.decode_png(image)\n",
    "\n",
    "                                              \n",
    "    def __decode_image__(self, image):\n",
    "        image_tensor = tf.convert_to_tensor(image, dtype=tf.int32)\n",
    "        return tfio.experimental.color.rgb_to_bgr(image_tensor)\n",
    "        #return tf.io.decode_image(np.asarray(image).astype('int32'))\n",
    "\n",
    "                                              \n",
    "    def __detect__(self, input_image):\n",
    "        \"\"\"Runs detection on a decoded input image.\n",
    "\n",
    "        Args:\n",
    "        input_image: A [1, height, width, 3] tensor represents the input image\n",
    "            pixels. Note that the height/width should already be resized and match the\n",
    "            expected input resolution of the model before passing into this function.\n",
    "\n",
    "        Returns:\n",
    "        A multi-dimensional float numpy array representing the predicted keypoint\n",
    "        coordinates and scores for the pose(s) detected in the image\n",
    "        \"\"\"\n",
    "        model = self.module.signatures['serving_default']\n",
    "\n",
    "        # SavedModel format expects tensor type of int32.\n",
    "        input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "        # Run model inference.\n",
    "        outputs = model(input_image)\n",
    "        # Output is a [1, 1, 17, 3] tensor for single pose\n",
    "        # Or a [1, 6, 56] tensor for multi pose (up to 6)\n",
    "        return outputs['output_0'].numpy()\n",
    "\n",
    "                                              \n",
    "    def __visualize_pose__(self, keypoints_with_scores):\n",
    "    \n",
    "        min_y = float(min(keypoints_with_scores[0, 0, :, 0]).numpy())\n",
    "        max_y = float(max(keypoints_with_scores[0, 0, :, 0]).numpy())\n",
    "        min_x = float(min(keypoints_with_scores[0, 0, :, 1]).numpy())\n",
    "        max_x = float(max(keypoints_with_scores[0, 0, :, 1]).numpy())\n",
    "\n",
    "        height = int(max_y - min_y + 1)\n",
    "        width = int(max_x - min_x + 1)\n",
    "        \n",
    "        aspect_ratio = float(width) / height\n",
    "\n",
    "        height_crop_ratio = 1\n",
    "        width_crop_ratio = 1\n",
    "        height_offset = min_y\n",
    "        width_offset = -min_x\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "        # To remove the huge white borders\n",
    "        fig.tight_layout(pad=0)\n",
    "        ax.margins(0)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        plt.axis('off')\n",
    "\n",
    "        im = np.zeros([height,width,3],dtype=np.uint8)\n",
    "        im.fill(255) # or img[:] = 255\n",
    "        ax.imshow(im)\n",
    "        line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "        ax.add_collection(line_segments)\n",
    "        # Turn off tick labels\n",
    "        scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "        (keypoint_locs, keypoint_edges,\n",
    "        edge_colors) = self.__keypoints_and_edges_for_display__(\n",
    "            keypoints_with_scores, 1, 1, height_crop_ratio, width_crop_ratio, height_offset, width_offset)\n",
    "        \n",
    "        line_segments.set_segments(keypoint_edges)\n",
    "        line_segments.set_color(edge_colors)\n",
    "        if keypoint_edges.shape[0]:\n",
    "            line_segments.set_segments(keypoint_edges)\n",
    "            line_segments.set_color(edge_colors)\n",
    "        if keypoint_locs.shape[0]:\n",
    "            scat.set_offsets(keypoint_locs)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        \n",
    "\n",
    "    def __visualize_normalized_pose__(self, keypoints_with_scores):\n",
    "    \n",
    "        min_y = float(min(keypoints_with_scores[0, 0, :, 0]).numpy())\n",
    "        max_y = float(max(keypoints_with_scores[0, 0, :, 0]).numpy())\n",
    "        min_x = float(min(keypoints_with_scores[0, 0, :, 1]).numpy())\n",
    "        max_x = float(max(keypoints_with_scores[0, 0, :, 1]).numpy())\n",
    "        \n",
    "#         print(min_x, max_x, min_y, max_y)\n",
    "#         height_crop_ratio = 1 / max_y\n",
    "#         width_crop_ratio = 1 / max_x\n",
    "#         height_offset = -min_y\n",
    "#         width_offset = -min_x\n",
    "        \n",
    "        height = 128\n",
    "        width = 128\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        # To remove the huge white borders\n",
    "        fig.tight_layout(pad=0)\n",
    "        ax.margins(0)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        plt.axis('off')\n",
    "\n",
    "        im = np.zeros([height,width,3],dtype=np.uint8)\n",
    "        im.fill(255) # or img[:] = 255\n",
    "        ax.imshow(im)\n",
    "        line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "        ax.add_collection(line_segments)\n",
    "        # Turn off tick labels\n",
    "        scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "        (keypoint_locs, keypoint_edges,\n",
    "        edge_colors) = self.__keypoints_and_edges_for_display__(\n",
    "            keypoints_with_scores, height, width) #, height_crop_ratio, width_crop_ratio, height_offset, width_offset)\n",
    "        \n",
    "        line_segments.set_segments(keypoint_edges)\n",
    "        line_segments.set_color(edge_colors)\n",
    "        if keypoint_edges.shape[0]:\n",
    "            line_segments.set_segments(keypoint_edges)\n",
    "            line_segments.set_color(edge_colors)\n",
    "        if keypoint_locs.shape[0]:\n",
    "            scat.set_offsets(keypoint_locs)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        \n",
    "        \n",
    "    def __visualize_detections__(self, keypoints_with_scores, image_tensor, rescale_keypoints=False, unitize_keypoints=True):\n",
    "        # Visualize the predictions on the image\n",
    "        image_height, image_width, _ = image_tensor.shape\n",
    "        display_image = tf.expand_dims(image_tensor, axis=0)\n",
    "\n",
    "        visual_keypoints_with_scores = np.copy(keypoints_with_scores)\n",
    "        \n",
    "        output_overlay = None\n",
    "        \n",
    "        if len(keypoints_with_scores[0][0]) > 0:\n",
    "            if unitize_keypoints:\n",
    "                # Convert back to unit positions (0-1) rather than pixel positions\n",
    "                for idx in range(17):\n",
    "                    visual_keypoints_with_scores[0, 0, idx, 0] = keypoints_with_scores[0, 0, idx, 0] / image_height\n",
    "                    visual_keypoints_with_scores[0, 0, idx, 1] = keypoints_with_scores[0, 0, idx, 1] / image_width\n",
    "\n",
    "            keypoint_scores_array = visual_keypoints_with_scores[0][0]\n",
    "\n",
    "            if rescale_keypoints:\n",
    "                max_dim = max(image_height, image_width)\n",
    "                display_image = tf.cast(tf.image.resize_with_pad(\n",
    "                    display_image, image_height, image_width), dtype=tf.int32)\n",
    "                output_overlay = self.__draw_predictions_on_image__(\n",
    "                    np.squeeze(display_image.numpy(), axis=0), keypoint_scores_array, output_image_height=image_height, output_image_width=image_width)\n",
    "            else:\n",
    "                output_overlay = self.__draw_predictions_on_image__(\n",
    "                    image_tensor.numpy(), keypoint_scores_array, output_image_height=image_height, output_image_width=image_width)\n",
    "\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        if output_overlay is not None:\n",
    "            plt.imshow(output_overlay)\n",
    "        _ = plt.axis('off')\n",
    "\n",
    "                                              \n",
    "    def visualize_detections(self, detections, image=None, normalized=False):\n",
    "        \"\"\"Visualization front end. Unlike the internal methods, this expects the image\n",
    "           input to be a standard WxHx3 array, and the keypoint locations and confidence\n",
    "           scores to be a Nx17x3 matrix (N=number of figures, usually 1)\n",
    "        \"\"\"\n",
    "\n",
    "        keypoint_scores_array = detections[0].data\n",
    "        \n",
    "        keypoints_with_scores = tf.expand_dims(tf.expand_dims(keypoint_scores_array, axis=0), axis=0)\n",
    "    \n",
    "        if image is not None:\n",
    "            self.__visualize_detections__(keypoints_with_scores, self.__decode_image__(image))\n",
    "        else:\n",
    "            if normalized:\n",
    "                self.__visualize_normalized_pose__(keypoints_with_scores)\n",
    "            else:\n",
    "                self.__visualize_pose__(keypoints_with_scores)\n",
    "\n",
    "                \n",
    "    def __get_frame_data__(self, im, crop_region=None, timecode=None, frame_count=1, images_too=False):\n",
    "        \"\"\"This code is duplicated between detect_webcam and detect_video\"\"\"\n",
    "        image_tensor = self.__decode_image__(im)\n",
    "\n",
    "        # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "        display_image = tf.expand_dims(image_tensor, axis=0)\n",
    "        display_image = tf.cast(tf.image.resize_with_pad(display_image, self.input_size, self.input_size), dtype=tf.int32)\n",
    "\n",
    "        image_height, image_width, _ = image_tensor.shape\n",
    "\n",
    "        if crop_region is None:\n",
    "            crop_region = self.__init_crop_region__(image_height, image_width)\n",
    "\n",
    "        keypoints_with_scores, pose_confidence_scores = self.__run_inference__(image_tensor, crop_region, crop_size=[self.input_size, self.input_size])\n",
    "        crop_region = self.__determine_crop_region__(keypoints_with_scores, image_height, image_width)\n",
    "\n",
    "        detections = [Detection(detection) for detection in keypoints_with_scores[0]]\n",
    "\n",
    "        this_frame_data = {'frame_id': frame_count, 'time': timecode, 'figures': detections, 'confidences': pose_confidence_scores, 'image_height': image_height, 'image_width': image_width} #, 'flipped_figures': flipped_detections, 'zeroified_figures': zeroified_detections}\n",
    "        if images_too:\n",
    "            this_frame_data['image'] = im\n",
    "            \n",
    "        return [this_frame_data, crop_region]\n",
    "        \n",
    "                                              \n",
    "    def detect_image(self, image_path, viz=False):\n",
    "        image_tensor = self.__decode_image_file__(image_path)\n",
    "        \n",
    "        im = cv2.imread(image_path)\n",
    "        \n",
    "        crop_region = None\n",
    "        \n",
    "        #image_tensor = self.__decode_image__(image)\n",
    "        \n",
    "        # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "        display_image = tf.expand_dims(image_tensor, axis=0)\n",
    "        display_image = tf.cast(tf.image.resize_with_pad(display_image, self.input_size, self.input_size), dtype=tf.int32)\n",
    "\n",
    "        image_height, image_width, _ = image_tensor.shape\n",
    "\n",
    "        if crop_region is None:\n",
    "            crop_region = self.__init_crop_region__(image_height, image_width)\n",
    "\n",
    "        keypoints_with_scores, pose_confidence_scores = self.__run_inference__(image_tensor, crop_region, crop_size=[self.input_size, self.input_size])\n",
    "        #crop_region = self.__determine_crop_region__(keypoints_with_scores, image_height, image_width)\n",
    "\n",
    "        if viz:\n",
    "            detections = [Detection(coords_and_scores) for coords_and_scores in keypoints_with_scores[0]]\n",
    "            image_plot = self.draw_predictions_on_image(im, detections, pose_confidence_scores)\n",
    "            display_img_array(image_plot)\n",
    "\n",
    "        return [keypoints_with_scores, pose_confidence_scores]\n",
    "\n",
    "                                              \n",
    "    def detect_webcam(self, max_frames=0, images_too=False):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        video_framerate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        print('video FPS:',video_framerate)\n",
    "        frame_duration = 1 / float(video_framerate)\n",
    "\n",
    "        frame_count = 0.0\n",
    "        frames_processed = 0\n",
    "        timecode = 0.0\n",
    "        \n",
    "        crop_region = None\n",
    "        pose_output = []\n",
    "        \n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "\n",
    "                ret_val, im = cap.read()\n",
    "\n",
    "                timecode = frame_count * frame_duration\n",
    "                frame_count += 1\n",
    "            \n",
    "                if (max_frames and frames_processed >= max_frames):\n",
    "                    return pose_output\n",
    "\n",
    "                this_frame_data, crop_region = self.__get_frame_data__(im, crop_region, timecode, frame_count, images_too)\n",
    "                    \n",
    "                # Periodically display detections during capture\n",
    "                if frame_count % 10 == 0:\n",
    "                    clear_output(wait=True)\n",
    "                    image_plot = self.draw_predictions_on_image(im, this_frame_data['figures'], this_frame_data['confidences'])\n",
    "                    display_img_array(image_plot)\n",
    "            \n",
    "                #self.visualize_detections([detection], im)\n",
    "                #break\n",
    "\n",
    "                pose_output.append(this_frame_data)\n",
    "                frames_processed += 1\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Capture stopped\")\n",
    "            \n",
    "        cap.release()\n",
    "        plt.close('all')\n",
    "                \n",
    "        return pose_output\n",
    "        \n",
    "    \n",
    "    def detect_video(self, video_file, start_seconds=0.0, end_seconds=0.0, max_frames=0, seconds_to_skip=0.0, images_too=False, write_images=False, folder_name='video_folder'):\n",
    "        \"\"\" Given a video file, extracts video frames as images at `seconds_to_skip` intervals,\n",
    "            from `start_seconds` to `end_seconds`, and runs `__detect_one_or_more_images__()` on each.\n",
    "            Returns a list of frame pose data items, which are dictionaries with the following elements:\n",
    "            { 'frame_id': <the frame's position in this list (not in the entire video, if seconds_to_skip != 0)>, \n",
    "              'time': <the frame's timecode within the excerpt (not within the full video, if start_seconds != 0)>,\n",
    "              'keypoints': <keypoints with scores>,\n",
    "              <OPTIONAL> 'image': <a PIL image object for the frame>\n",
    "            }\n",
    "            `write_images`, if true, causes the extracted frame images to be written to a folder\n",
    "            specified by `folder_name`, with the naming scheme `image00001.png`\n",
    "        \"\"\"\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print('total frames in video:',total_frames)\n",
    "\n",
    "        video_framerate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        print('video FPS:',video_framerate)\n",
    "        frame_duration = 1 / float(video_framerate)\n",
    "\n",
    "        frame_count = 0.0\n",
    "        frames_processed = 0\n",
    "        timecode = 0.0\n",
    "        skip_until = start_seconds\n",
    "\n",
    "        crop_region = None\n",
    "        pose_output = []\n",
    "\n",
    "        if write_images:\n",
    "            if not os.path.isdir(folder_name):\n",
    "                os.mkdir(folder_name)\n",
    "            for filename in os.listdir(folder_name):\n",
    "                file_path = os.path.join(folder_name, filename)\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "\n",
    "        bar = display(self.__progress__(0, total_frames-1), display_id=True)\n",
    "        while cap.isOpened() and (frame_count < total_frames):\n",
    "            ret_val, im = cap.read()\n",
    "\n",
    "            timecode = frame_count * frame_duration\n",
    "            frame_count += 1\n",
    "\n",
    "            bar.update(self.__progress__(frame_count, total_frames-1))\n",
    "\n",
    "            if (end_seconds and timecode > end_seconds) or (max_frames and frames_processed >= max_frames):\n",
    "                return pose_output\n",
    "\n",
    "            if timecode < start_seconds:\n",
    "                continue\n",
    "\n",
    "            if im is None:\n",
    "                # Might want to retry here\n",
    "                # print(\"Missed a frame, continuing...\")\n",
    "                # For now, we'll count a missed frame as a processed frame\n",
    "                continue\n",
    "\n",
    "            if seconds_to_skip and timecode < skip_until:\n",
    "                continue\n",
    "            else:\n",
    "                skip_until += seconds_to_skip\n",
    "\n",
    "            frame_id = int(round(cap.get(1)))\n",
    "\n",
    "            this_frame_data, crop_region = self.__get_frame_data__(im, crop_region, timecode, frame_count, images_too)\n",
    "            \n",
    "            if write_images:\n",
    "                output_image = self.draw_predictions_on_image(im, this_frame_data['figures'], this_frame_data['confidences'])\n",
    "                # Image doesn't necessarily come in as RGB(A)!\n",
    "                #rgbim = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGBA)\n",
    "                pil_image = PIL.Image.fromarray(output_image)\n",
    "                pil_image.save(os.path.join(folder_name, 'image' + str(int(frames_processed + 1)).zfill(5) + '.png'), 'PNG')\n",
    "\n",
    "            pose_output.append(this_frame_data)\n",
    "            frames_processed += 1\n",
    "\n",
    "        return pose_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teddy = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teddy.init_model(model_name=\"movenet_multipose_lightning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = teddy.detect_image(\"../tiktok/pose_excerpts/BTS_Practice.png\",viz=True)\n",
    "#detections = teddy.detect_image(\"sample_data/sample1.png\",viz=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pose_output = teddy.detect_video(\"../theater_videos/Einstein.mp4\", start_seconds=6310, end_seconds=7288, write_images=True, images_too=False)\n",
    "pickle.dump(pose_output, open(\"../theater_videos/Einstein_Ballet_6310_to_7288.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS = 25\n",
    "!ffmpeg -y -framerate $FPS -pattern_type glob -i 'Einstein_ballet_6310_6885/*.png' -strict '-2' -c:v libx264 -vf \"fps=$FPS\" -pix_fmt yuv420p Einstein_ballet_6310_6885.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teddy.init_model()\n",
    "detections = teddy.detect_image(\"sample_data/sample1.png\",viz=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(165, 215, 5):\n",
    "    teddy.visualize_detections(pose_output[i]['figures'], pose_output[i]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pose_output = pickle.load(open(\"../tiktok/pose_data/enayy_04_w_images.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pose_output = teddy.detect_video(\"../tiktok/boymeetsale.mp4\", write_images=False, images_too=False)\n",
    "pickle.dump(pose_output, open(\"../tiktok/pose_data/boymeetsale.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_capture = teddy.detect_webcam(images_too=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teddy.visualize_detections(pose_output[1030]['figures'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choreo_k.visualize import viz_dist_matrices\n",
    "viz_dist_matrices(pose_output[50], pose_output[50], figure_type='figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_plot = teddy.draw_predictions_on_image(pose_output[50]['image'], pose_output[50]['figures'], blank_background=True)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(image_plot)\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teddy.gif_from_detections(pose_output, fps=30, blank_background=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for fp in os.listdir(\"../tiktok/pose_excerpts\"):\n",
    "    if fp.endswith(\".png\"):\n",
    "        print(fp)\n",
    "        detections = teddy.detect_image(\"../tiktok/pose_excerpts/\" + fp,viz=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teddy.visualize_detections(pose_output[59]['figures'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Detector.detect_image)\n",
    "show_doc(Detector.detect_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
