{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize\n",
    "\n",
    "> Tools to visualize detected/corrected poses and their matrix representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import openpifpaf\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "#from IPython.display import display\n",
    "from skbio.stats.distance import mantel\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "  action='ignore', module='matplotlib.figure', category=UserWarning,\n",
    "  message=('This figure includes Axes that are not compatible with tight_layout, '\n",
    "           'so results might be incorrect.'))\n",
    "\n",
    "\n",
    "# Note: If an image is supplied, the detections are expected to be non-flipped\n",
    "# Otherwise, they should be flipped.\n",
    "def plot_poses(detections, image=None, show=True, savepath=\"\", show_axis=True):\n",
    "\n",
    "    skeleton_painter = openpipaf.show.KeypointPainter(color_connections=True, linewidth=6, highlight_invisible=True)\n",
    "    # Older verions of Open PifPaf:\n",
    "    #skeleton_painter = openpifpaf.show.KeypointPainter(\n",
    "    #  show_box=True, color_connections=True, markersize=1, linewidth=6, highlight_invisible=True, show_joint_scale=True)\n",
    "\n",
    "    if hasattr(detections, 'data'):\n",
    "        vis_detections = [detections]\n",
    "    else:\n",
    "        for pose in detections:\n",
    "            if pose.data.shape[0] == 0:\n",
    "                continue\n",
    "            vis_detections.append(pose)\n",
    "\n",
    "    with openpifpaf.show.canvas() as ax:\n",
    "        if image is not None:\n",
    "            ax.imshow(image)\n",
    "        else:\n",
    "            ax.set_aspect('equal')\n",
    "        skeleton_painter.annotations(ax, vis_detections)\n",
    "        if show:\n",
    "            openpifpaf.show.canvas()\n",
    "        if not show_axis:\n",
    "            ax.set_axis_off()\n",
    "        fig = ax.get_figure()\n",
    "        if savepath != \"\":\n",
    "            fig.savefig(savepath)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_delaunay(figure, image=None, show=True, show_axis=True):\n",
    "\n",
    "    if hasattr(figure, 'data'):\n",
    "        vis_figures = [figure]\n",
    "    else:\n",
    "        vis_figures = []\n",
    "        for pose in figures:\n",
    "            if pose.data.shape[0] == 0:\n",
    "                continue\n",
    "        vis_figures.append(pose)\n",
    "\n",
    "    for figure in vis_figures:\n",
    "        all_points = figure.data\n",
    "\n",
    "        # For visualization, remove all [x,y,0] (unknown) coordinates.\n",
    "        nonzero = (all_points!=0).all(axis=1)\n",
    "        nz_points = all_points[nonzero]\n",
    "        points = nz_points[:,:2]\n",
    "        total_points = len(points)\n",
    "        tri = Delaunay(points)\n",
    "\n",
    "        plt.triplot(points[:,0], points[:,1], tri.simplices.copy())\n",
    "        plt.plot(points[:,0], points[:,1], 'o')\n",
    "    \n",
    "    if image is not None:\n",
    "        plt.gca().imshow(image)\n",
    "    else:\n",
    "        plt.gca().set_aspect('equal')\n",
    "    if not show_axis:\n",
    "        plt.gca().set_axis_off()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "\n",
    "# From http://www.icare.univ-lille1.fr/tutorials/convert_a_matplotlib_figure\n",
    "def fig2img(fig2, w=8, h=8, dpi=72):\n",
    "\n",
    "    fig2.dpi=dpi\n",
    "    fig2.set_size_inches(w, h)\n",
    "    fig2.tight_layout()\n",
    "    fig2.gca().set_anchor('NE')\n",
    "\n",
    "    fig2.canvas.draw()\n",
    "\n",
    "    # Get the RGBA buffer from the figure\n",
    "    w,h = fig2.canvas.get_width_height()\n",
    "    buf = np.frombuffer(fig2.canvas.tostring_argb(), dtype=np.uint8)\n",
    "    buf.shape = (w,h,4)\n",
    "\n",
    "    # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode\n",
    "    buf = np.roll(buf, 3, axis=2)\n",
    "\n",
    "    # put the figure pixmap into a numpy array\n",
    "    w, h, d = buf.shape\n",
    "    im = Image.frombytes(\"RGBA\", (w,h), buf)\n",
    "    return im\n",
    "\n",
    "\n",
    "from choreo_k.modify import zeroify_detections, flip_detections, shift_figure\n",
    "\n",
    "\n",
    "# For visualizing individual poses with detection overlays\n",
    "def excerpt_pose(video_file, frame_poses, figure_index=0, show=False, plot_type='pose', source_figure='figures', flip_figures=False, margin=.2, width=None, height=None, show_axis=True):\n",
    "  \n",
    "  if source_figure not in frame_poses or len(frame_poses[source_figure]) == 0:\n",
    "    return None\n",
    "\n",
    "  figures_frame = copy.deepcopy(frame_poses)\n",
    "    \n",
    "  figures_frame['zeroified_figures'] = zeroify_detections(figures_frame[source_figure], width=width, height=height)\n",
    "\n",
    "  if flip_figures:\n",
    "    figures_frame['zeroified_figures'] = flip_detections(figures_frame['zeroified_figures'])\n",
    "\n",
    "  # This is used to cut out the background image, so it must be in the original (non-zeroified) coordinates\n",
    "  bbox = get_bbox(figures_frame[source_figure][figure_index].data, False, margin=margin, width=width, height=height)\n",
    "  figures_frame['zeroified_figures'][figure_index].data = shift_figure(figures_frame['zeroified_figures'][figure_index].data, bbox['marg'], bbox['marg'])\n",
    "    \n",
    "  cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "  total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "  video_framerate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "  timecode = figures_frame['time']\n",
    "\n",
    "  frameno = int(round(timecode * video_framerate))\n",
    "      \n",
    "  if frameno > total_frames:\n",
    "    print(frameno,\"IS GREATER THAN TOTAL FRAMES IN VIDEO:\",total_frames)\n",
    "    return None\n",
    "\n",
    "  cap.set(cv2.CAP_PROP_POS_FRAMES, frameno)\n",
    "  ret_val, im = cap.read()\n",
    "    \n",
    "  # Image doesn't necessarily come in as RGB(A)!\n",
    "  rgbim = cv2.cvtColor(im, cv2.COLOR_BGR2RGBA)\n",
    "  pil_image = PIL.Image.fromarray(rgbim)\n",
    "\n",
    "  cropped_image = pil_image.crop((bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']))\n",
    "  if plot_type == 'delaunay':\n",
    "    fig = plot_delaunay(figures_frame['zeroified_figures'][figure_index], cropped_image, show=show, show_axis=show_axis)\n",
    "  else:\n",
    "    fig = plot_poses(figures_frame['zeroified_figures'][figure_index], cropped_image, show=show, show_axis=show_axis)\n",
    "    \n",
    "  return fig\n",
    "\n",
    "\n",
    "# Overlay all detected poses in a single frame on the full image\n",
    "# (For multi-dancer videos)\n",
    "def overlay_poses(pil_image, figures_frame, show=False, plot_type='pose', source_figure='figures', show_axis=False, savepath=\"\"):\n",
    "\n",
    "    if plot_type == 'delaunay':\n",
    "        fig = plot_delaunay(figures_frame[source_figure], pil_image, show=show, show_axis=show_axis)\n",
    "    else:\n",
    "        fig = plot_poses(figures_frame[source_figure], pil_image, show=show, show_axis=show_axis, savepath=savepath)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "GC_INTERVAL = 1000\n",
    "\n",
    "def overlay_video(video_file, pose_data, plot_type='pose', source_figure='figures', show_axis=False, savedir=\"\", start_frame=0):\n",
    "    \"\"\" Set savedir to a folder where a whole bunch of images from the video, with\n",
    "        pose overlays drawn on them, will be stored. These can be turned into\n",
    "        a video (poses_video.mp4 or similar) later using this command:\n",
    "        !ffmpeg -y -framerate $FPS -pattern_type glob -i 'savedir/*.png' -strict '-2' -c:v libx264 -vf \"fps=$FPS\" -pix_fmt yuv420p poses_video.mp4\n",
    "        Note that both occurrences of $FPS should be replaced with the framerate of\n",
    "        the video, which can be obtained from get_video_stats(video_filename)\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_framerate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    for figures_frame in pose_data[start_frame:len(pose_data)]:\n",
    "\n",
    "        timecode = figures_frame['time']\n",
    "        frameno = int(round(timecode * video_framerate))\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frameno)\n",
    "        ret_val, im = cap.read()\n",
    "\n",
    "        # Image doesn't necessarily come in as RGB(A)!\n",
    "        rgbim = cv2.cvtColor(im, cv2.COLOR_BGR2RGBA)\n",
    "        pil_image = PIL.Image.fromarray(rgbim)\n",
    "\n",
    "        savepath = os.path.join(savedir, 'image' + str(frameno+1).zfill(5) + '.png')\n",
    "\n",
    "        fig = overlay_poses(pil_image, figures_frame, source_figure=source_figure, savepath=savepath)\n",
    "\n",
    "        del im, rgbim, pil_image\n",
    "\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "        plt.close(fig)\n",
    "        if frameno % GC_INTERVAL == 0:\n",
    "            gc.collect()\n",
    "    \n",
    "        del fig\n",
    "\n",
    "MIN_MOVE = 200\n",
    "MAX_MOVE = 1200\n",
    "\n",
    "def draw_figure(point_weights=None, show=True):\n",
    "    \"\"\" Scale keypoint radii by how much they moved in a video \"\"\"\n",
    "    links = [[0, 1], [0, 2], [1, 2], [1, 3], [2, 4], [3, 5], [4, 6], [5, 6],\n",
    "           [5, 7], [6, 8], [7, 9], [8, 10], [5, 11], [6, 12], [11, 12],\n",
    "           [11, 13], [12, 14], [13, 15], [14, 16]]\n",
    "    coords = [[160, 510],\n",
    "            [175, 525],\n",
    "            [145, 525],\n",
    "            [200, 520],\n",
    "            [120, 520],\n",
    "            [215, 440],\n",
    "            [105, 440],\n",
    "            [260, 335],\n",
    "            [60, 335],\n",
    "            [285, 215],\n",
    "            [35, 215],\n",
    "            [200, 280],\n",
    "            [120, 280],\n",
    "            [200, 150],\n",
    "            [120, 150],\n",
    "            [200, 25],\n",
    "            [120, 25]]\n",
    "\n",
    "    xcoords = [c[0] for c in coords]\n",
    "    ycoords = [c[1] for c in coords]\n",
    "\n",
    "    if point_weights.any():\n",
    "        input_weights = np.copy(point_weights)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    input_weights *= (200/input_weights.min())\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    cm = plt.cm.get_cmap('RdYlBu_r')\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.scatter(x=xcoords, y=ycoords, s=input_weights, c=input_weights, vmin=MIN_MOVE, vmax=MAX_MOVE, cmap=cm)\n",
    "\n",
    "    for link in links:\n",
    "        ax.plot([coords[link[0]][0], coords[link[1]][0]], [coords[link[0]][1], coords[link[1]][1]], 'k-')\n",
    "\n",
    "    ax.set_xlim([-35, 340])\n",
    "    ax.set_ylim([-20, 570])\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Distance matrix-based comparison tests\n",
    "from choreo_k.matrixify import get_pose_matrix\n",
    "\n",
    "def viz_dist_matrices(p1, p2, figure_type='flipped_figures'):\n",
    "    dmatrix1 = squareform(get_pose_matrix(p1, figure_type=figure_type))\n",
    "    dmatrix2 = squareform(get_pose_matrix(p2, figure_type=figure_type))\n",
    "    \n",
    "    plt.xticks(np.arange(17))\n",
    "    plt.yticks(np.arange(17))\n",
    "\n",
    "    plt.imshow(dmatrix1, cmap='viridis', origin='upper')\n",
    "    plot_poses(p1[figure_type][0].data)\n",
    "\n",
    "    plt.xticks(np.arange(17))\n",
    "    plt.yticks(np.arange(17))\n",
    "\n",
    "    plt.imshow(dmatrix2, cmap='viridis', origin='upper')\n",
    "    plot_poses(p2[figure_type][0].data)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    print(\"Similarity:\",mantel(dmatrix1, dmatrix2)[0])\n",
    "\n",
    "    diffmatrix = np.absolute(dmatrix1 - dmatrix2)\n",
    "    movers = diffmatrix.sum(axis=1)\n",
    "    plt.xticks(np.arange(17))\n",
    "    plt.yticks(np.arange(17))\n",
    "    plt.imshow(diffmatrix, cmap='viridis', origin='upper')\n",
    "    plt.figure()\n",
    "    plt.xticks(np.arange(17))\n",
    "    plt.bar(range(17), movers)\n",
    "    print(movers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
